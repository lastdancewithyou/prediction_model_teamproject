{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch_Sequence_Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pitch_sequence_parser\n",
    "import argparse\n",
    "\n",
    "def pitch_sequence_parser():\n",
    "    parser = argparse.ArgumentParser(description= \"Experiment for Pitch Sequence Classification\")\n",
    "    \n",
    "    ## seed\n",
    "    parser.add_argument('--seed', type = int, default = 42)\n",
    "    \n",
    "    ## data\n",
    "    parser.add_argument('--data_name', type = str, default = 'pybaseball_data_cleaned_v2')\n",
    "    parser.add_argument('--target_list', type = list, default = ['pitch_name', 'target_loc'])\n",
    "    parser.add_argument('--use_col_list', type = list, default = ['pitch_name', 'release_speed', 'release_pos_x', 'release_pos_z',\n",
    "       'pitcher', 'batter', 'zone', 'balls', 'strikes', 'pfx_x', 'pfx_z',\n",
    "       'plate_x', 'plate_z', 'outs_when_up', 'vx0', 'vy0', 'vz0', 'ax', 'ay',\n",
    "       'az', 'effective_speed', 'release_spin_rate', 'release_extension',\n",
    "       'release_pos_y', 'pitch_number', 'spin_axis', 'inning', 'game_pk',\n",
    "       'at_bat_number', 'score_diff', 'pitch_type_CS', 'pitch_type_CU',\n",
    "       'pitch_type_EP', 'pitch_type_FA', 'pitch_type_FC', 'pitch_type_FF',\n",
    "       'pitch_type_FO', 'pitch_type_FS', 'pitch_type_KC', 'pitch_type_KN',\n",
    "       'pitch_type_PO', 'pitch_type_SC', 'pitch_type_SI', 'pitch_type_SL',\n",
    "       'pitch_type_ST', 'pitch_type_SV', 'stand_R', 'p_throws_R', 'on_3b_1',\n",
    "       'on_2b_1', 'on_1b_1', 'score_diff', 'inning', 'cumulative_obp', 'cumulative_slg', 'cumulative_ops',\n",
    "       'game_obp', 'game_slg', 'game_ops', 'winning', 'losing', 'tied', 'tight' , 'low_zone_ratio'\n",
    "       'n_ff', 'n_si', 'n_fc', 'n_sl', 'n_ch',\t'n_cu','n_fs','n_kn','n_st','n_sv', 'target_loc'])\n",
    "    parser.add_argument('--pitch_name_list', type = list, default = ['4-Seam Fastball', 'Sinker', 'Slider', 'Changeup', 'Cutter', 'Sweeper', 'Curveball',\n",
    "                       'Split-Finger', 'Knuckle Curve'])\n",
    "    parser.add_argument('--seq_len', type = int, default = 16)\n",
    "    parser.add_argument('--data_path', type = str, default = './download_data')\n",
    "    parser.add_argument('--scaler_mode', type = str, default = 'standard')\n",
    "    \n",
    "    parser.add_argument('--model', type = str, default = 'invT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Files Exists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_files_exist(data_path):\n",
    "    files = [\n",
    "        os.path.join(data_path, 'seq.pkl'),\n",
    "        os.path.join(data_path, 'targets.pkl')\n",
    "    ]\n",
    "    return all(os.path.exists(f) for f in files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Pitch Frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pitch_frequencies(df, pitch_types):\n",
    "    \"\"\"\n",
    "    Calculates the pitch type frequency for each pitcher and adds new columns for each pitch type frequency.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing pitcher and pitch type dummy variables.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with added columns representing pitch type frequencies for each pitcher.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 각 투수의 구종 빈도 계산\n",
    "    pitcher_pitch_counts = df.groupby('pitcher')[pitch_types].sum()\n",
    "\n",
    "    # 투수별 총 투구 수 계산\n",
    "    pitcher_total_pitches = pitcher_pitch_counts.sum(axis=1)\n",
    "\n",
    "    # 각 투수별 구종 비율 계산\n",
    "    pitcher_pitch_frequencies = pitcher_pitch_counts.div(pitcher_total_pitches, axis=0)\n",
    "\n",
    "    # 구종 비율 컬럼 이름을 변경하여 pitch_freq_로 시작하도록 함\n",
    "    pitcher_pitch_frequencies.columns = [f'pitch_freq_{ptype.split(\"_\")[2]}' for ptype in pitch_types]\n",
    "\n",
    "    # 원래 데이터프레임에 구종 비율을 병합\n",
    "    df = df.merge(pitcher_pitch_frequencies, on='pitcher', how='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Sequence Pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    'release_speed', 'release_pos_x', 'release_pos_z', 'pfx_x', 'pfx_z', 'plate_x', 'plate_z',\n",
    "    'vx0', 'vy0', 'vz0', 'ax', 'ay', 'az', 'effective_speed', 'release_spin_rate', 'release_extension', \n",
    "    'release_pos_y', 'spin_axis', 'score_diff', 'outs_when_up', 'inning', 'balls', 'strikes','at_bat_number', \n",
    "    'pitch_number','zone', 'game_obp', 'game_slg', 'game_ops', 'cumulative_obp', 'cumulative_slg', 'cumulative_ops', 'low_zone_ratio'\n",
    "]\n",
    "encoding_columns = [\n",
    "    'pitch_name', 'target_loc', 'winning', 'losing', 'tied', 'tight', 'p_throws_R', 'on_3b_1', 'on_2b_1',\n",
    "      'on_1b_1','n_ff', 'n_si', 'n_fc', 'n_sl', 'n_ch', 'n_cu','n_fs', 'n_kn', 'n_st', 'n_sv',\n",
    "        'batter', 'pitcher'  # batter랑 pitcher는 시험삼아 넣어봄\n",
    "]\n",
    "\n",
    "# 업데이트된 target_columns (예측 대상으로 사용할 타겟 설정)\n",
    "target_columns = ['pitch_name', 'target_loc']\n",
    "\n",
    "\n",
    "def divide_sequence_pitch(df, pitch_num_col='pitch_number', target_columns=['pitch_name', 'target_loc'], max_seq_len=None):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    total_pitches = 0\n",
    "    ignored_pitches = 0\n",
    "\n",
    "    # 타석 단위로 그룹핑: game_pk, at_bat_number, inning 기준으로 그룹핑하여 타석 단위로 나눔\n",
    "    player_group = df.groupby(['game_pk', 'at_bat_number', 'inning', 'pitcher', 'batter'], observed=True)\n",
    "\n",
    "    for _, group in tqdm(player_group, desc=\"Processing at-bats\", total=len(player_group)):  \n",
    "        group = group.sort_values(by=pitch_num_col)\n",
    "        \n",
    "        total_pitches += len(group)\n",
    "\n",
    "        temp_sequence = []\n",
    "        temp_target = []\n",
    "        \n",
    "        for i in range(len(group)):\n",
    "            temp_sequence.append(group.iloc[i][numeric_columns + encoding_columns].values)\n",
    "            temp_target.append(group.iloc[i][target_columns].values)\n",
    "\n",
    "            if len(temp_sequence) >= max_seq_len:\n",
    "                sequences.append(temp_sequence[:max_seq_len])\n",
    "                targets.append(temp_target[:max_seq_len])\n",
    "                ignored_pitches += len(temp_sequence) - max_seq_len\n",
    "                temp_sequence = []\n",
    "                temp_target = []\n",
    "\n",
    "        if len(temp_sequence) > 0:\n",
    "            if len(temp_sequence) > max_seq_len:\n",
    "                ignored_pitches += len(temp_sequence) - max_seq_len\n",
    "                temp_sequence = temp_sequence[:max_seq_len]\n",
    "                temp_target = temp_target[:max_seq_len]\n",
    "\n",
    "            sequences.append(temp_sequence)\n",
    "            targets.append(temp_target)\n",
    "\n",
    "    ignored_ratio = ignored_pitches / total_pitches * 100 if total_pitches > 0 else 0\n",
    "    print(f\"Total pitches: {total_pitches}, Ignored pitches: {ignored_pitches}, Ignored ratio: {ignored_ratio:.2f}%\")\n",
    "\n",
    "    return sequences, targets, ignored_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatCastDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from utils.args import pitch_sequence_parser\n",
    "from utils.utils import all_files_exist, divide_sequence_pitch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from collections import Counter\n",
    "\n",
    "class StatcastDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_path,\n",
    "                 data_name,\n",
    "                 flag,\n",
    "                 target_list = ['pitch_name', 'target_loc'],\n",
    "                 use_col_list = ['pitch_name', 'release_speed', 'release_pos_x', 'release_pos_z',\n",
    "                                 'pitcher', 'batter', 'zone', 'balls', 'strikes', 'pfx_x', 'pfx_z',\n",
    "                                 'plate_x', 'plate_z', 'outs_when_up', 'vx0', 'vy0', 'vz0', 'ax', 'ay',\n",
    "                                 'az', 'effective_speed', 'release_spin_rate', 'release_extension',\n",
    "                                 'release_pos_y', 'pitch_number', 'spin_axis', 'inning', 'game_pk',\n",
    "                                 'at_bat_number', 'score_diff', 'pitch_type_CS', 'pitch_type_CU',\n",
    "                                 'pitch_type_EP', 'pitch_type_FA', 'pitch_type_FC', 'pitch_type_FF',\n",
    "                                 'pitch_type_FO', 'pitch_type_FS', 'pitch_type_KC', 'pitch_type_KN',\n",
    "                                 'pitch_type_PO', 'pitch_type_SC', 'pitch_type_SI', 'pitch_type_SL',\n",
    "                                 'pitch_type_ST', 'pitch_type_SV', 'stand_R', 'p_throws_R', 'on_3b_1',\n",
    "                                 'on_2b_1', 'on_1b_1', 'score_diff', 'inning', 'cumulative_obp', 'cumulative_slg', 'cumulative_ops',\n",
    "                                 'game_obp', 'game_slg', 'game_ops', 'winning', 'losing', 'tied', 'tight' , 'low_zone_ratio',\n",
    "                                 'n_ff', 'n_si', 'n_fc', 'n_sl', 'n_ch', 'n_cu','n_fs','n_kn','n_st','n_sv', 'target_loc'],\n",
    "                 pitch_name_list = ['4-Seam Fastball', 'Sinker', 'Slider', 'Changeup', 'Cutter', 'Sweeper', 'Curveball',\n",
    "                       'Split-Finger', 'Knuckle Curve'],\n",
    "                 scaler_mode = 'standard',\n",
    "                 train_size = 0.8,\n",
    "                 val_size = 0.2\n",
    "                 ):\n",
    "        self.data_path = data_path\n",
    "        self.data_name = data_name\n",
    "        self.target_list = target_list\n",
    "        self.use_columns = use_col_list\n",
    "        self.pitch_types = pitch_name_list\n",
    "        self.scaler_mode = scaler_mode\n",
    "        self.flag = flag\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        \n",
    "        self.sequence_data_path = os.path.join('./data', self.data_name)\n",
    "        \n",
    "        if not os.path.exists(self.sequence_data_path) or not all_files_exist(self.sequence_data_path):\n",
    "            self.__read_data__()\n",
    "            self.__preprocess__()\n",
    "            self.__devide_sequence__()\n",
    "            self.__pad_and_split_sequence__()\n",
    "            \n",
    "        else:\n",
    "            with open(os.path.join(self.sequence_data_path, \"seq.pkl\"), \"rb\") as f:\n",
    "                self.sequences = pickle.load(f)\n",
    "                \n",
    "            with open(os.path.join(self.sequence_data_path, \"targets.pkl\"), \"rb\") as g:\n",
    "                self.targets = pickle.load(g)\n",
    "                \n",
    "            self.__pad_and_split_sequence__()\n",
    "    #init은 새로운 칼럼들 추가 외에 수정 없음.         \n",
    "    def __read_data__(self):\n",
    "        df_raw = pd.read_csv(os.path.join(self.data_path, self.data_name + '.csv'))\n",
    "\n",
    "        # pitcher 칼럼을 정수형 인코딩으로 변환\n",
    "        convert_lst = ['pitcher', 'batter', 'game_pk']\n",
    "        for col in convert_lst:\n",
    "            df_raw[col] = df_raw[col].astype('category').cat.codes\n",
    "        # df_raw['pitch_name'] = df_raw['pitch_name'].astype('category').cat.codes\n",
    "\n",
    "        # 필요 칼럼만 필터링\n",
    "        df_raw = df_raw[self.use_columns]\n",
    "        #self.numeric_columns = [col for col in df_raw.columns if col not in ['pitcher', 'batter']]\n",
    "        self.int_chr_columns = ['inning', 'zone', 'balls', 'strikes', 'outs_when_up', 'pitch_number']\n",
    "        self.numeric_columns = numeric_columns\n",
    "        self.encoding_columns = [col for col in df_raw.columns if col.startswith('pitch_type_') or col.startswith('p_throws_') or col.startswith('stand_') or col.startswith('n_') or col.startswith('on_')]\n",
    "        self.cont_columns = [col for col in self.numeric_columns if col not in self.int_chr_columns and col not in self.encoding_columns and col != 'pitch_name']\n",
    "\n",
    "        self.data = df_raw.copy()  # 처리된 데이터 그대로 저장\n",
    "        \n",
    "\n",
    "    def __preprocess__(self):\n",
    "        # 수치형 데이터 정규화\n",
    "        if self.scaler_mode == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif self.scaler_mode == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "        else:\n",
    "            raise Exception(\"Scaler must be standardscaler or minmaxscaler\")\n",
    "\n",
    "        self.data[self.cont_columns] = self.scaler.fit_transform(self.data[self.cont_columns])\n",
    "        \n",
    "    def __devide_sequence__(self):\n",
    "        # Pitch type에 따른 dictionary 생성 및 mapping\n",
    "        self.pitch_mapping = {pitch: idx for idx, pitch in enumerate(self.pitch_types, start=0)}\n",
    "        self.pitch_mapping['Others'] = len(self.pitch_mapping)\n",
    "        self.data['pitch_name'] = self.data['pitch_name'].map(self.pitch_mapping).fillna(self.pitch_mapping['Others'])\n",
    "        \n",
    "        # Pitch location에 따른 dictionary 생성 및 mapping\n",
    "        location = ['in_high', 'out_high', 'in_low', 'out_low']\n",
    "        self.loc_mapping = {location: idx for idx, location in enumerate(location, start = 0)}\n",
    "        self.data['target_loc'] = self.data['target_loc'].map(self.loc_mapping)\n",
    "        \n",
    "        ## target 뒤로 빼고 정렬\n",
    "        use_col_list = [col for col in self.use_columns if col not in ['pitch_name', 'target_loc']] + ['pitch_name', 'target_loc']\n",
    "        self.data = self.data[use_col_list]\n",
    "        \n",
    "        print(self.pitch_mapping)\n",
    "        \n",
    "        print(self.loc_mapping)\n",
    "        \n",
    "        self.sequences, self.targets = divide_sequence_pitch(self.data,\n",
    "                                                             numeric_columns=self.numeric_columns,\n",
    "                                                             encoding_columns=self.encoding_columns,\n",
    "                                                             pitch_num_col='pitch_number', \n",
    "                                                             target_columns=self.target_list)\n",
    "        \n",
    "        # numpy 저장\n",
    "        os.makedirs(self.sequence_data_path, exist_ok = True)\n",
    "        \n",
    "        with open(os.path.join(self.sequence_data_path, \"seq.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.sequences, f)\n",
    "        \n",
    "        with open(os.path.join(self.sequence_data_path, \"targets.pkl\"), \"wb\") as g:\n",
    "            pickle.dump(self.targets, g)\n",
    "            \n",
    "        with open(os.path.join(self.sequence_data_path, \"pitch_mapping.pkl\"), 'wb') as h:\n",
    "            pickle.dump(self.pitch_mapping, h)\n",
    "            \n",
    "        with open(os.path.join(self.sequence_data_path, \"loc_mapping.pkl\"), 'wb') as i:\n",
    "            pickle.dump(self.loc_mapping, i)\n",
    "        \n",
    "        \n",
    "    def __pad_and_split_sequence__(self):\n",
    "        # 전체 sequence 패딩 처리\n",
    "        seq_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in self.sequences]\n",
    "        self.padded_sequences = pad_sequence(seq_tensors, batch_first=True, padding_value=0.0).permute(0, 2, 1)\n",
    "        self.max_seq_len = self.padded_sequences.shape[-1]  # 모든 세트에서 동일한 max_seq_len을 사용\n",
    "\n",
    "        # 시퀀스 길이와 패딩 마스크 생성\n",
    "        self.real_sequence_length = torch.tensor([len(seq) for seq in self.sequences])\n",
    "        self.padding_mask = torch.zeros(self.padded_sequences.shape[0], self.max_seq_len, dtype=torch.bool)\n",
    "        for i, length in enumerate(self.real_sequence_length):\n",
    "            self.padding_mask[i, length:] = True\n",
    "\n",
    "        # 타겟을 텐서로 변환\n",
    "        self.target_tensors = torch.stack([torch.tensor(tar_seq, dtype=torch.int) for tar_seq in self.targets], axis=0)\n",
    "\n",
    "        # train, valid, test 나누기\n",
    "        total_length = len(self.sequences)\n",
    "        train_valid_length = int(total_length * self.train_size)\n",
    "        val_length = int(train_valid_length * self.val_size)\n",
    "        train_length = train_valid_length - val_length\n",
    "\n",
    "        if self.flag == 'train':\n",
    "            self.padded_sequences = self.padded_sequences[:train_length]\n",
    "            self.real_sequence_length = self.real_sequence_length[:train_length]\n",
    "            self.padding_mask = self.padding_mask[:train_length]\n",
    "            self.target_tensors = self.target_tensors[:train_length]\n",
    "\n",
    "        elif self.flag == 'valid':\n",
    "            self.padded_sequences = self.padded_sequences[train_length:train_valid_length]\n",
    "            self.real_sequence_length = self.real_sequence_length[train_length:train_valid_length]\n",
    "            self.padding_mask = self.padding_mask[train_length:train_valid_length]\n",
    "            self.target_tensors = self.target_tensors[train_length:train_valid_length]\n",
    "\n",
    "        elif self.flag == 'test':\n",
    "            self.padded_sequences = self.padded_sequences[train_valid_length:]\n",
    "            self.real_sequence_length = self.real_sequence_length[train_valid_length:]\n",
    "            self.padding_mask = self.padding_mask[train_valid_length:]\n",
    "            self.target_tensors = self.target_tensors[train_valid_length:]\n",
    "            \n",
    "        ## 범주형 빈도 리스트\n",
    "        pitch_type_count = [0] * 10\n",
    "        pitch_location_count = [0] * 4\n",
    "        \n",
    "        for target in self.target_tensors[: train_length]:\n",
    "            pitch_type_count[int(target[0])] += 1\n",
    "            pitch_location_count[int(target[1])] += 1\n",
    "            \n",
    "        ## 빈도를 1에서 빼서 비율 계산\n",
    "        self.pitch_type_freq = [1 - (i / sum(pitch_type_count)) for i in pitch_type_count]\n",
    "        self.pitch_location_freq = [1 - (i / sum(pitch_location_count)) for i in pitch_location_count]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.padded_sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_dict = {\n",
    "            \"padded_data\": self.padded_sequences[index],\n",
    "            \"real_sequence_length\": self.real_sequence_length[index],\n",
    "            \"padding_mask\": self.padding_mask[index],\n",
    "            \"targets\": self.target_tensors[index]\n",
    "        }\n",
    "        return data_dict\n",
    "    \n",
    "    def _return_weight(self):\n",
    "        return self.pitch_type_freq, self.pitch_location_freq\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        \"\"\"\n",
    "        정규화 역변환\n",
    "        \"\"\"\n",
    "        return self.scaler.inverse_transform(data)\n",
    "    \n",
    "    \n",
    "def get_loader(args, flag):\n",
    "    statcast_dataset = StatcastDataset(\n",
    "        data_name = args.data_name,\n",
    "        data_path = args.data_path,\n",
    "        target_list = args.target_list,\n",
    "        use_col_list = args.use_col_list,\n",
    "        pitch_name_list = args.pitch_name_list,\n",
    "        scaler_mode = args.scaler_mode,\n",
    "        flag = flag,\n",
    "        train_size = 0.8,\n",
    "        val_size = 0.2\n",
    "    )\n",
    "    \n",
    "    # print(statcast_dataset.pitch_type_dictionary())\n",
    "    \n",
    "    shuffle = False\n",
    "\n",
    "    if flag == 'train':\n",
    "        shuffle = True\n",
    "    \n",
    "    statcast_loader = DataLoader(\n",
    "        statcast_dataset,\n",
    "        batch_size = args.batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = True\n",
    "    )\n",
    "    \n",
    "    if flag == 'train':\n",
    "        type_weight, location_weight = statcast_dataset._return_weight()\n",
    "        return statcast_dataset, statcast_loader, type_weight, location_weight\n",
    "        \n",
    "    return statcast_dataset, statcast_loader\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = pitch_sequence_parser()\n",
    "    args = parser.parse_args()\n",
    "    dataset, loader = get_loader(args, flag = 'train')\n",
    "    print(next(iter(loader))['padded_data'].shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
