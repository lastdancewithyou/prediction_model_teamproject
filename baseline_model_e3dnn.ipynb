{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_data = pd.read_csv(\"/Users/hoengbird/Downloads/pybaseball_data_cleaned_v2.csv\")\n",
    "our_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_parquet(\"/Users/hoengbird/Downloads/pybaseball_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = raw_data[['pitch_name', 'release_speed', 'release_pos_x', 'release_pos_z',\\\n",
    "                        'pitcher', 'batter', 'zone', 'balls', 'strikes', 'pfx_x', 'pfx_z',\\\n",
    "                        'plate_x', 'plate_z', 'outs_when_up', 'at_bat_number', 'pitch_number',\\\n",
    "                        'post_bat_score', 'post_away_score', 'home_team', 'away_team', 'game_date',\\\n",
    "                        'inning_topbot', 'inning', 'on_1b', 'on_2b', 'on_3b', 'type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_data.dropna(subset=['pitch_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타순 계산 함수\n",
    "def calculate_batting_order(group):\n",
    "\n",
    "    home_order_list = []\n",
    "    away_order_list = []\n",
    "\n",
    "    current_home_order = 0\n",
    "    current_away_order = 0\n",
    "    prev_top_at_bat = 0  # 이전 팀의 공격(at_bat_number 기준)\n",
    "\n",
    "    # 이닝별 처리\n",
    "    for _, row in group.iterrows():\n",
    "        if row['inning_topbot'] == 'Top':  # 어웨이 팀 공격\n",
    "            current_away_order = (current_away_order % 9) + 1\n",
    "            away_order_list.append(current_away_order)\n",
    "            home_order_list.append(0)\n",
    "            prev_top_at_bat = row['at_bat_number']  # 어웨이팀 at_bat_number 업데이트\n",
    "        elif row['inning_topbot'] == 'Bot':  # 홈 팀 공격\n",
    "            # 현재 at_bat_number에서 이전 어웨이팀 at_bat_number를 빼고 계산\n",
    "            relative_at_bat = row['at_bat_number'] - prev_top_at_bat\n",
    "            current_home_order = (relative_at_bat % 9) + 1\n",
    "            home_order_list.append(current_home_order)\n",
    "            away_order_list.append(0)\n",
    "\n",
    "    group['away_order'] = away_order_list\n",
    "    group['home_order'] = home_order_list\n",
    "    group['order'] = group['away_order'] + group['home_order']\n",
    "    return group\n",
    "\n",
    "grouped = filtered_data.groupby(['game_date', 'home_team', 'away_team'], group_keys=False)\n",
    "progress_bar = tqdm(grouped, total=len(grouped), desc=\"Processing Groups\")\n",
    "\n",
    "result = pd.concat([calculate_batting_order(group) for _, group in progress_bar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ = result[['inning', 'outs_when_up', 'release_speed', 'release_pos_x', 'release_pos_z', 'order', 'type']]\n",
    "data = our_data.merge(merge_, on=['inning', 'outs_when_up', 'release_speed', 'release_pos_x', 'release_pos_z'], how='left')\n",
    "\n",
    "# order 문제 처리 완료될 경우 본 코드는 삭제함\n",
    "data.dropna(subset=['order'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_dataset = data[['inning', 'on_1b_1', 'on_2b_1', 'on_3b_1', 'balls', 'strikes', 'outs_when_up', 'winning', 'losing', 'tied',\\\n",
    "                        'stand_R', 'zone', 'pitch_name', 'order', 'type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dnn_dataset.loc[:, 'on_1b_1'] = dnn_dataset.loc[:, 'on_1b_1'].notnull().astype(int)\n",
    "# dnn_dataset.loc[:, 'on_1b_1'] = dnn_dataset.loc[:, 'on_2b_1'].notnull().astype(int)\n",
    "# dnn_dataset.loc[:, 'on_1b_1'] = dnn_dataset.loc[:, 'on_3b_1'].notnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_dataset = dnn_dataset.rename(columns={'on_1b_1':'Base1', 'on_2b_1':'Base2', 'on_3b_1':'Base3', 'balls':'Ball', 'strikes':'Strike', 'stand_R':'LR', 'outs_when_up':'Out', 'order':'Order', 'inning': 'InnNum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strike, ball 구분\n",
    "## 타격(X)일 경우 strike 처리\n",
    "dnn_dataset['T_Strike'] = dnn_dataset['type'].apply(\n",
    "    lambda x: 1 if x in ['X', 'S'] else 0\n",
    ")\n",
    "\n",
    "dnn_dataset['T_Ball'] = dnn_dataset['type'].apply(\n",
    "    lambda x: 1 if x=='B' else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target pitch type processing\n",
    "dnn_dataset['Fastball'] = dnn_dataset['pitch_name'].apply(\n",
    "    lambda x: 1 if x in ['4-Seam Fastball', 'Sinker'] else 0\n",
    ")\n",
    "\n",
    "dnn_dataset['Nonfastball'] = dnn_dataset['pitch_name'].apply(\n",
    "    lambda x: 1 if x not in ['4-Seam Fastball', 'Sinker'] else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target location processing\n",
    "print(dnn_dataset['zone'].unique())\n",
    "print(\"전체 투구: \", len(dnn_dataset))\n",
    "dnn_dataset = dnn_dataset[dnn_dataset['zone'] < 10]\n",
    "print(\"스트라이크 존만 필터링: \", len(dnn_dataset))\n",
    "\n",
    "dnn_dataset.loc[:, 'horizontal'] = dnn_dataset['zone'].apply(\n",
    "    lambda x: 'Left' if x in [1,4,7] else ('Center' if x in [2,5,8] else 'Right')\n",
    ")\n",
    "\n",
    "dnn_dataset.loc[:, 'vertical'] = dnn_dataset['zone'].apply(\n",
    "    lambda x: 'Up' if x in [1,2,3] else ('Middle' if x in [4,5,6] else 'Down')\n",
    ")\n",
    "\n",
    "dnn_dataset.loc[:, 'horizontal_Left'] = (dnn_dataset['horizontal']=='Left').astype(int)\n",
    "dnn_dataset.loc[:, 'horizontal_Center'] = (dnn_dataset['horizontal']=='Center').astype(int)\n",
    "dnn_dataset.loc[:, 'horizontal_Right'] = (dnn_dataset['horizontal']=='Right').astype(int)\n",
    "\n",
    "dnn_dataset.loc[:, 'vertical_Up'] = (dnn_dataset['vertical']=='Up').astype(int)\n",
    "dnn_dataset.loc[:, 'vertical_Middle'] = (dnn_dataset['vertical']=='Middle').astype(int)\n",
    "dnn_dataset.loc[:, 'vertical_Down'] = (dnn_dataset['vertical']=='Down').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좌타 우타 처리\n",
    "dnn_dataset['LR'] = dnn_dataset['LR'].apply(lambda x: 2 if x == 1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경기 상황 처리\n",
    "dnn_dataset['LDW'] = dnn_dataset.apply(lambda row: 3 if row['winning'] == 1 else (2 if row['tied'] == 1 else 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요 열 정리\n",
    "dnn_dataset = dnn_dataset.drop(columns=['winning', 'losing', 'tied', 'horizontal', 'vertical', 'zone', 'pitch_name', 'type'])\n",
    "dnn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2dnn_transform(row, columns):\n",
    "    mapping = {\n",
    "        (1, 0, 0): [1, 0],\n",
    "        (0, 1, 0): [0, 1],\n",
    "        (0, 0, 1): [1, 1]\n",
    "    }\n",
    "    one_hot_tuple = tuple(row[columns].values)\n",
    "    return mapping.get(one_hot_tuple, [None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_columns = [\"horizontal_Left\", \"horizontal_Center\", \"horizontal_Right\"]\n",
    "dnn_dataset[[\"H1\", \"H2\"]] = dnn_dataset.apply(e2dnn_transform, axis=1, columns=horizontal_columns).apply(pd.Series)\n",
    "\n",
    "vertical_columns = [\"vertical_Up\", \"vertical_Middle\", \"vertical_Down\"]\n",
    "dnn_dataset[[\"V1\", \"V2\"]] = dnn_dataset.apply(e2dnn_transform, axis=1, columns=vertical_columns).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_dataset.to_parquet('dnn_dataset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_dataset = pd.read_parquet(\"C:/Users/gangmin/Documents/카카오톡 받은 파일/dnn_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [\"InnNum\", \"LDW\", \"Base1\", \"Base2\", \"Base3\", 'Ball', \"Strike\", \"Out\", 'Order', \"LR\"]\n",
    "target_cols = [\"T_Strike\", \"Fastball\", \"H1\", \"H2\", \"V1\", \"V2\"]\n",
    "\n",
    "X = dnn_dataset[input_cols].values\n",
    "y = dnn_dataset[target_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitchDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.4)\n",
    "\n",
    "splits = []\n",
    "for train_index, temp_index in splitter.split(X, np.argmax(y, axis=1)):\n",
    "    temp_X, temp_y = X[temp_index], y[temp_index]\n",
    "    \n",
    "    val_test_split = StratifiedShuffleSplit(n_splits=1, test_size=0.5)\n",
    "    for val_index, test_index in val_test_split.split(temp_X, np.argmax(temp_y, axis=1)):\n",
    "        train_X, train_y = X[train_index], y[train_index]\n",
    "        val_X, val_y = temp_X[val_index], temp_y[val_index]\n",
    "        test_X, test_y = temp_X[test_index], temp_y[test_index]\n",
    "        \n",
    "        splits.append((train_X, train_y, val_X, val_y, test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSFNF_DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BSFNF_DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 15)\n",
    "        self.fc2 = nn.Linear(15, 20)\n",
    "        self.fc3 = nn.Linear(20, 25)\n",
    "        self.fc4 = nn.Linear(25, 30)\n",
    "        self.fc5 = nn.Linear(30, 35)\n",
    "        self.fc6 = nn.Linear(35, 30)\n",
    "        self.fc7 = nn.Linear(30, 25)\n",
    "        self.fc8 = nn.Linear(25, 15)\n",
    "        self.fc9 = nn.Linear(15, 8)\n",
    "        self.output = nn.Linear(8, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.relu(self.fc8(x))\n",
    "        x = F.relu(self.fc9(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "class HL_DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HL_DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 15)\n",
    "        self.fc2 = nn.Linear(15, 20)\n",
    "        self.fc3 = nn.Linear(20, 30)\n",
    "        self.fc4 = nn.Linear(30, 30)\n",
    "        self.fc5 = nn.Linear(30, 20)\n",
    "        self.fc6 = nn.Linear(20, 15)\n",
    "        self.fc7 = nn.Linear(15, 8)\n",
    "        self.output = nn.Linear(8, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "class VL_DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VL_DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 15)\n",
    "        self.fc2 = nn.Linear(15, 20)\n",
    "        self.fc3 = nn.Linear(20, 25)\n",
    "        self.fc4 = nn.Linear(25, 30)\n",
    "        self.fc5 = nn.Linear(30, 35)\n",
    "        self.fc6 = nn.Linear(35, 40)\n",
    "        self.fc7 = nn.Linear(40, 40)\n",
    "        self.fc8 = nn.Linear(40, 35)\n",
    "        self.fc9 = nn.Linear(35, 30)\n",
    "        self.fc10 = nn.Linear(30, 25)\n",
    "        self.fc11 = nn.Linear(25, 15)\n",
    "        self.fc12 = nn.Linear(15, 8)\n",
    "        self.output = nn.Linear(8, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.relu(self.fc8(x))\n",
    "        x = F.relu(self.fc9(x))\n",
    "        x = F.relu(self.fc10(x))\n",
    "        x = F.relu(self.fc11(x))\n",
    "        x = F.relu(self.fc12(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "class E3_DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(E3_DNN, self).__init__()\n",
    "        self.bsfnf = BSFNF_DNN()\n",
    "        self.hl = HL_DNN()\n",
    "        self.vl = VL_DNN()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bsf_nf_output = self.bsfnf(x)\n",
    "        hl_output = self.hl(x)\n",
    "        vl_output = self.vl(x)\n",
    "        \n",
    "        final_output = torch.cat((bsf_nf_output, hl_output, vl_output), dim=1)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_targets(targets):\n",
    "    # BSFNF_DNN: T_Strike, Fastball\n",
    "    bsf_targets = targets[:, :2]\n",
    "\n",
    "    # HL_DNN: H1, H2\n",
    "    hl_targets = targets[:, 2:4]\n",
    "\n",
    "    # VL_DNN: V1, V2\n",
    "    vl_targets = targets[:, 4:]\n",
    "\n",
    "    return bsf_targets, hl_targets, vl_targets\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_bsf = nn.BCELoss()\n",
    "criterion_hl = nn.BCELoss()\n",
    "criterion_vl = nn.BCELoss()\n",
    "\n",
    "def compute_loss(outputs, targets):\n",
    "    bsf_output = outputs[:, :2]\n",
    "    hl_output = outputs[:, 2:4]\n",
    "    vl_output = outputs[:, 4:]\n",
    "\n",
    "    bsf_targets, hl_targets, vl_targets = encode_targets(targets)\n",
    "\n",
    "    loss_bsf = criterion_bsf(bsf_output, bsf_targets)\n",
    "    loss_hl = criterion_hl(hl_output, hl_targets)\n",
    "    loss_vl = criterion_vl(vl_output, vl_targets)\n",
    "\n",
    "    return loss_bsf + loss_hl + loss_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, model, optimizer, num_epochs=1000, patience=3):\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", leave=True, dynamic_ncols=True, smoothing=0.1) as train_bar:\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = compute_loss(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                train_bar.update(1)\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\", leave=True, dynamic_ncols=True, smoothing=0.1) as val_bar:\n",
    "                    for inputs, targets in val_loader:\n",
    "                        inputs, targets = inputs.to(device), targets.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        val_loss += compute_loss(outputs, targets).item()\n",
    "                        val_bar.update(1)\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                early_stop_count = 0\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "\n",
    "            if early_stop_count >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_loader, model):\n",
    "    total_predictions = 0\n",
    "    correct_predictions = {\"BSFNF\": 0, \"HL\": 0, \"VL\": 0, \"Overall\": 0}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            bsf_output, hl_output, vl_output = outputs[:, :2], outputs[:, 2:4], outputs[:, 4:]\n",
    "            bsf_targets, hl_targets, vl_targets = encode_targets(targets)\n",
    "\n",
    "            bsf_pred = (bsf_output > 0.5).float()\n",
    "            hl_pred = (hl_output > 0.5).float()\n",
    "            vl_pred = (vl_output > 0.5).float()\n",
    "\n",
    "            correct_predictions[\"BSFNF\"] += (bsf_pred == bsf_targets).all(dim=1).sum().item()\n",
    "            correct_predictions[\"HL\"] += (hl_pred == hl_targets).all(dim=1).sum().item()\n",
    "            correct_predictions[\"VL\"] += (vl_pred == vl_targets).all(dim=1).sum().item()\n",
    "\n",
    "            overall_correct = (\n",
    "                (bsf_pred == bsf_targets).all(dim=1) &\n",
    "                (hl_pred == hl_targets).all(dim=1) &\n",
    "                (vl_pred == vl_targets).all(dim=1)\n",
    "            ).sum().item()\n",
    "\n",
    "            correct_predictions['Overall'] += overall_correct\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "    bsf_accuracy = correct_predictions['BSFNF'] / total_predictions\n",
    "    hl_accuracy = correct_predictions['HL'] / total_predictions\n",
    "    vl_accuracy = correct_predictions[\"VL\"] / total_predictions\n",
    "    overall_accuracy = correct_predictions['Overall'] / total_predictions\n",
    "\n",
    "    print(f\"BSFNF Accuracy: {bsf_accuracy:.4f}\")\n",
    "    print(f\"HL Accuracy: {hl_accuracy:.4f}\")\n",
    "    print(f\"VL Accuracy: {vl_accuracy:.4f}\")\n",
    "    print(f\"E3DNN Accuracy:, {overall_accuracy:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"BSFNF Accuracy\": bsf_accuracy,\n",
    "        \"HL Accuracy\": hl_accuracy,\n",
    "        \"VL Accuracy\": vl_accuracy,\n",
    "        \"Overall Accuracy\": overall_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for split_idx, split in enumerate(splits, 1):\n",
    "    print(f\"\\nProcessing Split {split_idx}/{len(splits)}\")\n",
    "\n",
    "    train_X, train_y, val_X, val_y, test_X, test_y = split\n",
    "    \n",
    "    train_loader = DataLoader(PitchDataset(train_X, train_y), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(PitchDataset(val_X, val_y), batch_size=16, shuffle=False)\n",
    "    test_loader = DataLoader(PitchDataset(test_X, test_y), batch_size=16, shuffle=False)\n",
    "\n",
    "    # 모델 초기화\n",
    "    model = E3_DNN()\n",
    "    model.apply(init_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # 학습\n",
    "    train_model(train_loader, val_loader, model, optimizer)\n",
    "\n",
    "    # 테스트\n",
    "    accuracies = evaluate_model(test_loader, model)\n",
    "    results.append(accuracies)\n",
    "\n",
    "bsfnf_avg = np.mean([r[\"BSFNF Accuracy\"] for r in results])\n",
    "hl_avg = np.mean([r[\"HL Accuracy\"] for r in results])\n",
    "vl_avg = np.mean([r[\"VL Accuracy\"] for r in results])\n",
    "overall_avg = np.mean([r[\"Overall Accuracy\"] for r in results])\n",
    "\n",
    "print(\"\\n=== split-wise Results ===\")\n",
    "for split_idx, accuracies in enumerate(results, 1):\n",
    "    print(f\"Split {split_idx}:\")\n",
    "    print(f\"BSFNF Accuracy: {accuracies['BSFNF Accuracy']:.4f}\")\n",
    "    print(f\"HL Accuracy: {accuracies['HL Accuracy']:.4f}\")\n",
    "    print(f\"VL Accuracy: {accuracies['VL Accuracy']:.4f}\")\n",
    "    print(f\"Overall Accuracy: {accuracies['Overall Accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"Average BSFNF Accuracy: {bsfnf_avg:.4f}\")\n",
    "print(f\"Average HL Accuracy: {hl_avg:.4f}\")\n",
    "print(f\"Average VL Accuracy: {vl_avg:.4f}\")\n",
    "print(f\"Average Overall Accuracy: {overall_avg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
